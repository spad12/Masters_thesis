%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Design Options}
\label{ch:design}

GPU architecture is significantly different than that of a CPU, and thus a high performance PIC code on a GPU is going to look a lot different from its CPU equivalent. Memory access patterns, cache behavior, thread communication, and thread workload all have significant impacts on the performance of GPU codes. This means that porting an existing PIC code to the GPU is by no means straightforward, the data structures and algorithms will likely be different from the original serial code. 

Performance is just one facet of the code design, maintaining separate CPU and GPU versions of the same code presents additional problems. Programmers tend to be lazy in that the fewer lines of code that they have to write, the better. If a new feature is desired, then two different implementations of that feature must be written and debugged. From the lazy programmers perspective this is to be avoided as much as possible. Therefore, it is very important that the GPU version of the code utilize as much of the CPU code as possible. This means that interoperability between the CPU and GPU code must be both efficient and fast. 

Performance and maintenance are the two key issues that were considered when designing sceptic3Dgpu. Some of these issues have been investigated previously, although the amount of research in this area is still very small. To make matters worse, the specific techniques used are rapidly evolving with every new generation of graphics card. It is unlikely that the pace of GPU hardware evolution will slow in the near future. Spending large amounts of time optimizing algorithms for the current generation of hardware is inadvisable, and therefore the design of the code should focus on utilizing techniques that emphasize the underlying principles of GPU design or utilize library functions that will be optimized for each generation of hardware. 

The goal of this chapter is to outline various design options for implementing the various steps of the PIC algorithm on the GPU and explore the pros and cons of each option. Solutions used by other researchers will be outlined and evaluated based on their applicability to sceptic3D and their applicability to PIC codes in general. To accelerate these evaluations a simple 3D sandbox PIC code was implemented on the GPU in addition to several other basic comparison codes. 

%%%%%%%%%%%
%This means that separate CPU and GPU versions of the code must be maintained depending on the features desired for each architecture. Maintaining two separate versions of a code and as such it is important that the focus of GPU PIC code development be focused on steps of the PIC algorithm that are the most computationally intensive. Steps should be taken during the development to 


%For a serial or even MPI PIC implementations there is enough memory that each thread can have a separate copy of the grid and only need to talk to one another after chugging through a massive list of particles. In these cases the communications costs are negligible compared to the computations performed by each thread. On the GPU the situation is a lot different. Instead of a few threads doing lots of work you have a lot of threads doing little work. 
%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{GPUPIC Sandbox and the big questions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first step in the development of sceptic3Dgpu was to create a very simple, generalized pic code that performed the major steps of the PIC algorithm and implement it in CUDA. This simple code, we'll call it GPUPIC\_testbed is designed without making any assumptions about the physics of the system. GPUPIC\_testbed operates in Cartesian coordinates with periodic boundary conditions. We do not really care too much about the field solve since in the serial version it takes a very small amount of time compared to the particle advance and charge assign steps. By recognizing the low priority of the field solve we really only need to characterize the performance of the following 5 steps:


\begin{enumerate}\itemsep0pt \parskip0pt \parsep0pt
\item Read the particle data
\item Read the Potential data for that particle
\item Move the particle
\item Write the new particle data back to the particle list
\item Interpolate Charge to Mesh
\item Goto 1
\end{enumerate}



The first implementation of this code was very naive. The only real difference from a serial version was the density array update, which used atomic updates on global memory in order to prevent memory collisions between multiple threads. Other than that the code boiled down to unrolling the loop over all of the particles into one particle per thread. The runtime breakdown of this code for a $32^3$ grid and 4.2 million particles is shown in table \ref{tab:GPUPIC_basetime}.

\begin{center}
\begin{table}
\begin{tabular}{| p{4.0cm} | p{3.5cm} |}
\hline
Component & Runtime (ms) \\ \hline
Particle data read, move, and write & 375 \\ \hline
Potential Grid Read & 467 \\ \hline
Charge Assign & 1.143e4  \\ \hline
Total & 1.227e4  \\ \hline
\end{tabular}
\caption{Total Execution times for 100 iterations of the key steps of the move kernel at three different optimizations.}
\label{tab:GPUPIC_basetime} 
\end{table}
\end{center}

As you can see, the particle move and the potential read are very similar, but the charge assign is very slow. Determining how we can better adapt the charge assign to the GPU is our first major challenge. Several ways of dealling with the issue of the charge assign will be discussed in the following section. Some of the other issues that will be discussed in this chapter are:

\begin{itemize}\itemsep0pt \parskip0pt \parsep0pt
\item Particle Data Structure: Is it better to use an Array of structures, like the fortran code, or a Structure of Arrays?
\item How do we handle divergent processes in the advancing routine, such as losses, reinjections, and collisions?
\item At what point does the field solve become a dominant cost?
\item Are there any new issues that arise from solutions to the other issues?
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Charge Assign}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are two different ways to approach the charge assign, one in which information is ``pulled" from the particles by the vertices, and one in which data is ``pushed" by the particles to the vertices. Let G represent a grid of domain D of dimension d comprised of all vertices $v_s  \in \mathrm{D}$. We can define some distribution function $f(v_s)$ at each of the vertices which is the sum of some function $\mathrm{K}(v_s,p_i)$, where $p_i$ is the position of particle $i$. Given these definitions the algorithms for the particle pull and particle push method are algorithms \ref{alg:particle_pull} and \ref{alg:particle_push} respectively. 

\begin{algorithm}
	\begin{algorithmic}
		\STATE // Loop over the verticies first
		\FORALL{$\mathrm{vertex} \: v_s \in G$}
			\STATE find $\mathcal{P}(v_s)$
			\STATE $\mathrm{f}(v_s) \leftarrow 0$
			\FORALL{$p_i \in \mathcal{P}(v_s)$}
			\STATE $\mathrm{f}(v_s) \leftarrow \mathrm(f)(v_s) + \mathrm{K}(v_s,p_i)$
			\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\caption{Particle Pull Method of charge deposition. From Stantchev et al. \cite{Stantchev2008}}
	\label{alg:particle_pull}
\end{algorithm}

\begin{algorithm}
	\begin{algorithmic}
		\STATE // Loop over the verticies first
		\FORALL{$\mathrm{vertex} \: v_s \in G$}
			\STATE $\mathrm{f}(v_s) \leftarrow 0$
		\ENDFOR
		\FORALL{$\mathrm{particle} \: p_i \in \mathrm{D}$}
			\STATE find $\mathcal{V}(p_i)$
			\FORALL{$v_s \in \mathcal{V}(p_i)$}
				\STATE $\mathrm{f}(v_s) \leftarrow \mathrm(f)(v_s) + \mathrm{K}(v_s,p_i)$
			\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\caption{Particle Push Method of charge deposition. From Stantchev et al. \cite{Stantchev2008}}
	\label{alg:particle_push}
\end{algorithm}

As pointed out by \cite{Stantchev2008} each method has its advantages and disadvantages. For an algorithm consisting of N particles and k grid vertices the advantages and disadvantages are as follows:

The particle pull method
\begin{itemize}
\item requires $\mathcal{O}(2^dN + k)$ read write operations
\item $\mathcal{P}(v_s)$ is expensive to retrieve dynamically unless particles are organized
\end{itemize}

The particle push method
\begin{itemize}
\item requires $\mathcal{O}((2^d+1)N)$ read/write operations
\item $\mathcal{V}(p_i)$ is easily computed dynamically from the particles coordinates
\end{itemize}

The charge assign that we implemented in the sandbox PIC code is a particle push to an array in global memory. In that implementation we used atomic opperations to prevent memory collisions. Looking back at table \ref{tab:GPUPIC_basetime} we notice that the charge assign step constitutes about 93\% of the total runtime.  Unfortunately this poor performance is a result of serialization caused by the atomic updates. Additionaly, since the grid is far too large to fit in shared memory these updates must be performed on global memory, which has much higher latency and lower bandwidth. When a thread attempts to update a value in memory and finds that it is locked it must then repeat the process until it succeeds. Every failed update represents an additional slow global memory access that is essentially wasted. 

\begin{figure}
\begin{center}
\includegraphics[width=4in]{introduction/not_finished.pdf}
\end{center}
\caption{Atomic Memory collisions}
\label{fig:pic_flowchart_parallel}
\end{figure}

The technique applied for MPI codes is parallel reduction. Each thread deals with a subset of the particle list and tallies up the contributions of that list to some array in memory private to a single thread. Once every thread has recorded the contributions from their subset of the particle list a parallel reduction is performed in order to quickly sum up the contributions from all threads. The problem with directly applying this solution to the GPU is that when a thread reads in a particle the thread must be able to account for every possible location that the particle can contribute to. With a completely random particle list any given particle can contribute to any element of the grid. However, say a thread knows that every particle that it reads in will only contribute to one element of the grid. This thread now only has to keep track of a single value, since it knows that every particle it sees will only contribute to this value. When it comes time for all of the threads to contribute to the final result each thread provides the full answer for a single element. This is essentially the particle pull method described in algorithm \ref{alg:particle_pull} without the need to to retrieve $\mathcal{P}(v_s)$ dynamically. One of the main benefits to this method is that it significantly reduces the memory requirements of each thread but imposes the constraint that a thread is given only particles that exist within its domain. We will worry about this additional constraint later. 

\begin{figure}
\begin{center}
\includegraphics[width=4in]{introduction/not_finished.pdf}
\end{center}
\caption{One thread per cell}
\label{fig:pic_flowchart_parallel}
\end{figure}

Now consider this, the MPI code works well for a few randomly ordered sets of many particles, or objects, manipulated by a small number of threads. The decomposition technique works for a small number of organized sets of a few objects manipulated a large number of threads. If we think of threads operating on small groups of particles as objects and we replace every instance of `objects' with `threads' in the previous two sentences we end up with an interesting situation. Apply the MPI technique to a few randomly ordered sets of many threads each operating on a small number of particles. Essentially if we want to run really large particle lists we can divide up the list amongst several nodes. Each node uses many threads to process a small ordered subset of this list and contribute to the full array. Once every node has completed its own tally the standard MPI technique is used to gather the tallies of all the nodes. This is an excellent example of multi-grained parallelism. The level consisting of multiple nodes is coarse parallelism while the node level is a finer level of parallelism.   

\begin{figure}
\begin{center}
\includegraphics[width=4in]{introduction/not_finished.pdf}
\end{center}
\caption{MPI and One thread per cell}
\label{fig:pic_flowchart_parallel}
\end{figure}

We can take this methodology even further on the GPU by recognizing that we can parallelize the single element summations using reductions. Taking this to the limit of one thread per particle on the GPU we end up with each thread block, or several blocks, is responsible for a subset of the particle list. All of the particles in the block's list will contribute to the same element. The threads within each block read in their particles contribution to that element into shared memory. With all of the data in shared memory a very fast parallel reduction can be performed.

\begin{figure}
\begin{center}
\includegraphics[width=4in]{introduction/not_finished.pdf}
\end{center}
\caption{Three levels of parallelism for the charge assign}
\label{fig:pic_flowchart_parallel}
\end{figure}

We implemented this technique in the sandbox PIC code and compared the runtime of the reduction particle-pull to the atomic particle-push. The results of this comparison can be seen in table \ref{tab:GPUPIC_comparison}.

\noindent \begin{table}
\begin{tabular}{| p{4.0cm} | p{3.5cm} | p{3.5cm} |}
\hline
Component & Atomic-Updates (ms) & Sorted+Reduction (ms) \\ \hline
Particle data read, move, and write & 375 & 468 \\ \hline
Potential Grid Read & 467 & 285 \\ \hline
Charge Assign & 1.143e4 & 542 \\ \hline
Particle List Sort & 0 & 2.305e3 \\ \hline
Total & 1.227e4 & 3600 \\ \hline
\end{tabular}
\caption{Total Execution times for 100 iterations of the key steps of the move kernel at two different optimizations.}
\label{tab:GPUPIC_comparison}
\end{table}
As you can see from the table, the charge assign is on the order of 20x faster using the reduction technique, although this speedup is somewhat offset by the sorting requirement. Sorting the particles also benefits reading the potential during the advancing step. This speedup is a result of increased cache hits due to all threads within the same thread block accessing the same addresses in the potential array. 

Although we have successfully reduced the cost of the charge assign we have introduced an additional cost of a sorting step\footnote{It should be noted that the sort used here is an older version of the radix sort, newer versions, such as the thrust implementation are much faster}. In the case of the sandbox code the sort step accounts for roughly 70\% of the runtime. Fortunately several other projects have figured out that there are ways reduce the sorting costs while maintaining some of the performance achieved by utilizing a sorted particle list. 

 
		\subsection{Other Codes}
There are several papers which point out that sorting by cell at every time step is not entirely necessary for the particle-pull method. It is possible to minimize the sorting requirement by expanding the sorting bins to include multiple cells, or rather, by dividing the simulation space into slabs composed of multiple cells. The advantage to this technique is sorting is only required between slabs, but not within the slabs.\cite{Abreu2011} 

This slab method, as described by Abreu et al, is used on a one thread per slab basis. One thread for each slab loops over all of the particles that belong to that slab, contributing to an array that is the same size as the slab. Once a thread completes its particle loop it writes the portion of the array that it is responsible for to the main array, using atomic operations for guard cells.\cite{Abreu2011} Similar approaches are used by Stantchev et al and Kong et al.\cite{Stantchev2008}\cite{Kong2011}

Unfortunately it is difficult to apply the reduction version of the particle push to the slab method. The reason this is difficult boils down to limited shared memory. Consider a slab with $nv_{slab}$ vertices. In order for the reduction to work we need to have $nv_{slab} * \mathrm{nthreads}$ floats to store the results of each thread. For a typical NVIDIA GPU with 49k shared memory per streaming multiprocessor and 128 threads per block, we are limited to a slab of about 96 vertices per slab. This amounts to 9 cells per slab for a 3D grid, or about 3 fewer steps for a radix sort.

The approaches used by Kong and Stantchev is a sort of hybridization of the push and pull algorithms. Here the grid is domain decomposed into sub-domains and each sub-domain assigned to a thread-block. Each thread-block has an array representing the distribution function for that sub-domain allocated in shared memory. Particles are ordered in the particle list according to what sub-domain they reside in. Within each sub-domain the charge assign is performed like a particle push. Threads loop through a subset of the particles, check which vertices that particle is updating, and update the distribution function at those vertices. 

In order to avoid memory collisions both Kong and Stantchev use a technique similar to atomic operations. The technique that they used is called thread-tagging and is no longer needed due to the addition of atomic operations for shared memory.\cite{Stantchev2008}\cite{Kong2011} This approach has several advantages over the reduction technique, the primary reasons being lower order sorting keys and slightly easier implementation. The disadvantage of this approach is that because it relies on atomic operations there is no guarantee that the results are deterministic since the order of the atomic operations is undefined. 







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Particle List Sort}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		In the previous section we discussed what is required for an efficient charge assign on the GPU. In order to massively parallelize the charge assign and avoid memory collisions the particle data must be organized. Unfortunately this means introducing a new step in the PIC method, a sort step. Looking back at table \ref{tab:GPUPIC_comparison} we can see that this sort step is now the dominant cost by a large margin\footnote{Please note that these results are for the radix sort outlined in the NVIDIA GPU computing SDK version 3.1, developed by N. Satish et al.\cite{Satish2009}}. Operating with one step consuming 64\% of the total run time is unacceptable, we need to figure out a better way of keeping the particle data organized than this radix sort. Fortunately this problem has been explored in great detail by just about everyone else who has developed a GPU PIC implementation. 

Particle sorting for GPU PIC codes basically comes in four flavors:
\begin{itemize}
\item Partial sort using message passing. \cite{Kong2011}\cite{Decyk2011}
\item In-place particle Quicksort. \cite{Stantchev2008}
\item Linked list reordering \cite{Burau2010}
\item Full Radix Sort-by-key and reorder. \cite{Abreu2011}
\end{itemize}
Each of these methods have their own advantages and disadvantages. For the purposes of this code we are looking for a method that is fast for a broad range of applications and does not depend too greatly on the specifics of the problem.



	\subsection{Message Passing}
	Going back a section to the charge assign we concluded that sorting by cell is unnecessary. Instead we are ordering the particles according to a group of cells called bins. For most cases the dimensions of the bin will be greater than the average distance that a particle will travel in a given time step. There are cases in which the particle path length will be smaller than the size of a cell, however this is much more likely if the domain in question is several cells wide. The benefit of considering this case is that only a small fraction of the particles will leave the domain during a given time step, and thus only a small number of particles need to be moved from one domain to the next. Most of the particles will remain in their respective domains and therefore do not need to be sorted. Instead of a full particle sort we want a method that will partially sort the particle list, only handling the particles that changed domains. 

	One partial sorting method is similar in principle to message passing. The particle list is divided up into sections according to domain. Whenever a particle leaves its current domain it is flagged. Flagged particles are then moved to different sections of the particle list through some kind of buffer. There are currently two approaches to this. The approach taken by Kong et al, illustrated in figure \ref{fig:kong_sort} is based on integrating the buffer into the particle list. The particle list is structured such that each sub-domain's section of the particle list is divided into two regions, a data region and a buffer region. Using this particle list structure as a foundation, the rest of the sorting algorithm proceeds as follows\cite{Kong2011}:

\begin{enumerate}
\singlespace
\item Sub-domains, referred to by Kong as clusters, that are adjacent horizontally are grouped into pairs called bi-clusters. This first step is odd cells on the left, even cells on the right. 
\item Particles that are moving from the left cluster to the right are copied from the left cluster into the buffer section of the right cluster. 
\item Step 2 is then repeated for particles moving from the right cluster to the left. 
\item Repeat steps 1 to 3 for bi-clusters for even cells on the left and odd cells on the right. 
\item Perform steps 1 to 4 for vertically oriented bi-clusters. 
\end{enumerate}


\begin{figure}
\begin{center}
\includegraphics[width=3in]{design/kong_sort.png}
\end{center}
\caption{Kong particle sort. (a) Bi-cluster of cells. (b) Data structure of the bi-cluster particle array. (c.1) Particle 3 and 5 move to the right. Particle 3 is moved first to slot 12 which frees up slot 3. Slot 3, now empty, copies the last particle in the data region, which is particle 5. Particle 5 is also moving to the right so slot 3 copies it to slot 13 and replaces the contents of slot 3 with particle 4. Image taken from \cite{Kong2011}.}
\label{fig:kong_sort}
\end{figure}

	In cases where the number of particles in a cluster is greater than the number of slots in that cluster a global data reorder must be performed. This reorder is only performed when a particle to slot ratio exceeds a certain limit. When a cluster exceeds this threshold the code increases the buffer for this cluster by reducing the buffer of other clusters. This operation is carried out through a sequence of calls to $cudaMemcpy()$ where a section of data from end of a cluster is copied to the end of the buffer of the previous cluster. The start index of the adjusted cluster is then shifted by the amount of memory copied. \cite{Kong2011} 

  The second approach used by Decyk et al breaks the relocating particles into two groups, those moving within a thread group and one for all the rest. The buffer for the second group is stored in a different array from the main particle list.

All of the flagged particles are condensed and written to some kind of buffer using stream compaction. Once all particles that are being relocated have been transferred to the buffer and all threads have been synchronized particles in the buffer array are broadcast to the empty slots in the primary array. 













\begin{figure}
\begin{center}
\includegraphics[width=5in]{design/sort_compare.pdf}
\end{center}
\caption{Comparison of Stantchev Sort and the thrust radix sort.}
\label{fig:stantchev_sort_compare}
\end{figure}

		\subsection{Costs and Benefits}
		\subsection{Other Codes}
		*Stantchev Particle Binning
		*Kong Particle Passing
		*Linked Particle List
		\subsection{In house tests}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Particle List Structure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
		\subsection{Other Codes}
		\subsection{In house tests}
\noindent \begin{table}[h]
\begin{tabular}{| p{4.0cm} | p{3.5cm} | p{2.5cm} | p{4.0cm} |}
\hline
Component & SoA (ms) & AoS (ms) & Speedup (SoA vs AoS) \\ \hline
Particle data read, move, and write & 758 & 955 & 1.26x \\ \hline
Count Particles & 32.7 & 109 & 3.35x \\ \hline
Data Reorder & 346 & 480 & 1.38x \\ \hline
Total CPU run time & 2491 & 3284 & 1.31x \\ \hline
\end{tabular}
\caption{Execution times of main steps for Array of Structures and Structure of Arrays. Count Particles and Data Reorder are steps used for a sorted particle list. Count Particles counts the number of particles in each sub-domain. Data Reorder reorders the particle list data after the binindex / particle ID pair have been sorted by the radix sort.}
\label{tab:struct_compare} 
\end{table}

	\section{Particle Advancing}
		\subsection{Assumptions}
		\subsection{Other Codes}
		\subsection{Reinjections and Diagnostics}

	\section{Poisson Solve}
		\subsection{Desired Performance}
		\subsection{Performance vs Implementation Difficulty}

	\section{Grid Dimension Constraints and Handling}
