%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Implementation} 
\label{ch:implementation}
	%In this chapter we will detail the steps taken to implement the methods discussed in chapter \ref{ch:design}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Constraining Grid Dimensions}
	\label{sec:grid_constraints}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
There are two constraints that the grid dimensions must conform to. The first is set by the requirements of a simple z-order curve, the second is set by the size of the on chip shared memory. We define $n_{virtual}$ the ratio of a grid dimension and a bin dimension. These constraints are expressed mathematically through the grid dimensions, $n_r, n_{\theta}, n_{\psi}$, and the bin sub-domain dimensions, $nb_r, nb_{\theta}, nb_{\psi}$. As shown in equation \ref{eqn:constraints}, the ratio in each direction must be equivalent and be a power of 2.
		
\begin{equation}
\frac{n_r}{nb_r} = \frac{n_{\theta}}{nb_{\theta}} = \frac{n_{\psi}}{nb_{\psi}} = n_{virtual}
\label{eqn:constraints}
\end{equation} 



The second constraint on the grid dimensions is set by the hardware. The goal is to maximize the shared-multiprocessor occupancy for the charge assign stage of the code. Given that each thread block has the maximum number of threads, 512, and each thread requires roughly 25 registers, the maximum number of thread blocks that can exist simultaneously on a single SM is 2. This means that each thread block can be allocated half of the total amount of shared memory on the SM. Compute capability 2.0 GPUs have 49152 bytes of shared memory per SM. Running two thread blocks per SM provides each block with 24576 bytes of shared memory each, or 6144 floats per block. The maximum that all three bin dimensions can be is 18. For the sake of simplicity these hardware constraints set $nb_r, nb_{\theta}$, and $nb_{\psi} \le 18$. For now none of the bin dimensions are allowed to exceed 18, even if another is smaller. 

A third, loose constraint can be set in order to force a minimum number of thread blocks for the charge assign. The command line option ``--minbins\#'' sets the parameter $n_{virtual} = \#$. This is useful in ensuring that enough thread blocks are launched to populate all of the SMs on the GPU. To populate all of the SMs on a GTX 470 the code would need to launch at least 28 thread blocks. For a GTX 580 with 16 SMs 32 thread blocks are required to fill all of the processors.  
		%\section{Holding to Grid Constraints}

	\section{Particle List Transpose}
As previously mentioned the particle list structure on the GPU is different than the structure on the CPU. On the GPU particles are stored in a structure of arrays, while on the CPU they are stored in a 6x$n$ array. This means that in order to copy a particle list generated on the CPU to the GPU, or vice versa, the particle list must be transposed. The two main places in the code where this matters is when the particle list is initially populated at the start of the code, and when copying a list of pre-calculated reinjection particles from the CPU to the GPU at every time step during the advancing phase.

The particle list transpose was implemented on the CPU in two different ways depending on the compiler used and the available libraries. A GPU based particle list transpose is significantly faster than a CPU based transpose. However, the GPU has a very limited amount of DRAM compared to the CPU, and it is preferable to use as much of the available GPU memory as possible for the main particle list. In any case transposing the entire particle list only occurs once, but a smaller transpose is performed every time step for reinjected particles. This means that while a faster transpose is preferable, it represents so little of the total computation time that it is not worth developing a complicated in place GPU transpose.  

	\section{Charge Assign}
	\label{sec:charge_assign}
	As previously mentioned, the charge assign is one of the most difficult functions to parallelize. The naive approach of applying a thread to every particle and atomically adding each particles contribution to an array in global memory is very slow. Grouping the particles spatially allows the majority of the atomic operations to be done in the context of shared memory which is much faster than global memory. The resulting algorithm resembles basic domain decomposition where each thread block represents a separate sub-domain. The actual charge deposition method in this scheme is very similar to the naive approach, with a key difference being that all the threads in the thread block are operating on shared memory. Once all particles in the sub-domain have contributed to the grid in shared memory it takes only a small number of global memory accesses to write the contributions of a large number of particles to the main array. 

	\subsection{Domain Decomposition}
		The primary grid is decomposed into sub-domains of size $nb_r, nb_{\theta}, nb_{\psi}$. The methods for determining the size of the sub-domains is outlined in section \ref{sec:grid_constraints}. The indexing of the sub-domains is done using a z-order curve in order to preserve spatial locality of the sub-domains in memory. This is done in an attempt to reduce the mean distance that particles must be moved in memory during the sort phase. A graphical representation of this is shown in figure \ref{fig:domain_decomp}.

\begin{figure}
\begin{center}
\includegraphics[width=5in]{implementation/zorder_sceptic.pdf}
\end{center}
\caption[ParticleBin Organization]{Graphical Representation of domain decomposition and ParticleBin organization. The domain is divided up such that every region, highlighted by alternating blue and green fill, is the same size. Additionally the number of bins in any given direction is a power of two. This figure shows what $4^3$ bins looks like.}
\label{fig:domain_decomp}
\end{figure}

In addition to representing a sub-section of the computational mesh, each sub-domain must have a section of the particle list associated with it. The sub-domain must know all of the particles that reside within the region defined by that sub-domain. In essence each sub-domain represents a bin of particles that corresponds to some spatial location, hence the use of ``ParticleBin"  as the naming convention for these object.



	\subsection{Particle Bins}
The \emph{ParticleBin} object keeps track of all of the particles that reside in the region of space that the \emph{ParticleBin} represents. For the sake of simplicity all of the particle bins are the same size spatially, which means that the \emph{ParticleBin} object only has to keep track of the section of the main particle list that the bin represents and the spatial origin of the bin. 

In the context of the particle list each bin represents a pair of bookmarks that bound a section of the particle list. The bookmarks for each bin are calculated after the particle list is sorted by algorithm \ref{alg:count_bins}.

\begin{algorithm}
	\caption{ParticleBin Bookmark Calculation}
	\label{alg:count_bins}
	\begin{algorithmic}
		\FORALL{  $\mathrm{threadID} = 0 \to \mathrm{ParticleList.nptcls}$ in \underline{parallel}}
			\STATE 
			\STATE $\mathrm{binID} = \mathrm{ParticleList.binID}[\mathrm{threadID}]$
			\STATE $\mathrm{binID_{left}} = \mathrm{ParticleList.binID}[\mathrm{threadID} - 1]$
			\STATE $\mathrm{binID_{right}} = \mathrm{ParticleList.binID}[\mathrm{threadID} + 1]$
			
			\IF{$\mathrm{binID} \neq \mathrm{binID_{left}}$}
				\STATE $\mathrm{ParticleBins[binID].ifirstp} = \mathrm{threadID}$
				\STATE $\mathrm{ParticleBins[binID_{left}].ilastp} = \mathrm{threadID} - 1$
			\ENDIF

			\IF{$\mathrm{binID} \neq \mathrm{binID_{right}}$}
				\STATE $\mathrm{ParticleBins[binID].ilastp} = \mathrm{threadID}$
				\STATE $\mathrm{ParticleBins[binID_{right}].ifirstp} = \mathrm{threadID} + 1$	
			\ENDIF
			
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


The spatial origin of the bin is hashed using a z-order curve and stored as a 16-bit unsigned-integer. This 16-bit integer is refered to as the binID and is used for determining the region of the domain that a bin is responsible for as well as a sorting key for the particle list. Calculating the binID will be discussed in more detail in section \ref{sec:plist_sort}. A 16-bit unsigned integer is used for several reasons. First the sorting method detailed in section \ref{sec:plist_sort} is dependent on the number of bits of the sorting key. Second, the upper bound on the grid size set by using a 16-bit integer to store the z-order hash is much larger than the largest grid size that would need to be run. For a 16-bit integer this upper bound is $512^3$ grid points. The third reason for using a 16-bit integer is that it also reduces the memory requirements of the particle list by about 5\%, which does help when trying to run as many particles on the GPU as possible. 

	\subsection{Particle Push}
Now that the particles are organized spatially in memory, it is trivial to assign a single thread block to a region of space and corresponding particle bin in order to perform the particle push. This process is rather simple and is outlined in psuedo code in algorithm \ref{alg:gpu_c2mesh}. 

\begin{algorithm}
	\caption{GPU Charge Assign}
	\label{alg:gpu_c2mesh}
	\begin{algorithmic}
		\FORALL{ $ParticleBin \in Grid$ in \underline{parallel}}
			\STATE \emph{$\backslash \backslash$ Inside the threadBlock with ID blockID}

			\STATE \_\_shared\_\_ float $subGrid(nb_r,nb_{\theta},nb_{\psi})$

			\FORALL{ $node \in subGrid$ in \underline{parallel}}
			\STATE $node = 0$
			\ENDFOR
			
			\STATE \_\_syncthreads()
			

			\FORALL{  $particle \in ParticleBin$ in \underline{parallel}}
				\STATE cell = $particle.cell - ParticleBin.origin$
				\FORALL{ $node \in cell$}
					\STATE atomicAdd($subGrid(cell,node)$, weight($node$))
				\ENDFOR
			\ENDFOR
			\STATE \_\_syncthreads()

			\STATE \emph{$\backslash \backslash$ Write block results to global memory}
			\FORALL{ $node \in subGrid$ in \underline{parallel}}
				\STATE atomicAdd($Grid(blockID,node)$, $subGrid(node)$)
			\ENDFOR
			
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

Each thread block reads in 512 particles at a time, although only 32 particles, a warp, are processed in parallel within the block. Each thread within this warp loops over the 8 nodes that bound the cell that contains the particle being processed. The nodes reside in a shared memory array, and are updated with the weighted particle data atomically. Once all of the nodes for a given particle have been updated the thread will retrieve a new particle from global memory. This process is repeated by all of the threads in the block until every particle in the particle bin has been processed. Once all of the particles have been processed the block then atomically updates the nodes in global memory with the values stored in shared memory.  

The atomic operations in this algorithm lead to some very interesting time complexity behavior. In essence this algorithm is being executed on a machine with 32 processors. The time complexity of this scenario is $\mathcal{O}(\frac{c}{p})$, where $c$ is constant and $p$ is the number of available processors. When two processors attempt to atomically update the same memory address, one of the processors must wait until the other is finished. This means that one processor is effectively lost for a 1-way conflict. 

The mean number of n-way atomic conflicts $N$ in a warp over a sub domain of size $G$, and the execution time $T$ is given by:
\begin{equation}
N = \frac{31!}{(31-n)! G^n} \hspace{0.1\textwidth} T(n) \propto \frac{c}{32-n}
\end{equation}

This means that the total time complexity of this algorithm with respect to the sub domain size $G$ is:

\begin{equation}
T(G) \propto c \cdot \sum_{n=1}^{31} \frac{1}{32-n} \frac{31!}{(31-n)! G^n}
\end{equation}

This behavior can be seen roughly in \ref{fig:subdomain_size_scan}. This algorithm on the GPU can perform the particle push up to 200x faster than the CPU version of the charge assign. However, this algorithm relies on the particle data being ordered spatially, which contributes to the run time. The method used to maintain an ordered particle list on the gpu will be discussed in the following section.


	\section{Particle List Sort}
	\label{sec:plist_sort}
	
	As previously mentioned in section \ref{sec:charge_assign} an ordered particle list must be maintained in order for the charge assign to be fast. The particle list sort, algorithm \ref{alg:sort_overview} consists of three distinct subroutines, populating the key/value pairs, sorting the key/value pairs, and finally a payload move. 

\begin{algorithm}
	\caption{Particle List Sort Overview}
	\label{alg:sort_overview}
	\begin{algorithmic}
		\STATE
		\STATE Populate\_KeyValues(Particles, Mesh, sort\_keys, sort\_values)
		\STATE
		\STATE thrust::sort\_by\_key(sort\_keys,sort\_keys+nptcls,sort\_values)
		\STATE
		\STATE Payload\_Move(Particles, sort\_values)
	\end{algorithmic}
\end{algorithm}

This method of maintaining particle list order was chosen because it is a good balance between simplicity and performance. An additional benefit of this routine is that it uses the sort from the \gls{gls:thrust} library, which is maintained by NVIDIA. 

		\subsection{Populating Key/Value Pairs}
The first step in sorting the particle list is ensuring that the key/value pairs needed by the sorting routine are populated. The sorting key for a particle is the index of the particle bin that the particle belongs to. Sorting values are simply the position of the particle in the unsorted list. 

Calculating the particle bin index, or binid, begins with calculating the mesh cell that the particle resides in. This cell described by coordinates $i_r, i_{\theta}, i_{\phi}$. The coordinates of the particle bin that a given cell resides in is given by:

\begin{equation}
ib_r = \frac{i_r}{nb_r} \mathrm{;}\hspace{0.5in} ib_{\theta} = \frac{i_{\theta}}{nb_{\theta}} \mathrm{;} \hspace{0.5in} ib_{\phi} = \frac{i_{\phi}}{nb_{\phi}}
\end{equation}

	The resulting block coordinates are then hashed using a z-order curve and stored as the binid. Each thread calculates the binid's for several particles and stores them in the sort\_keys array. Once a thread has calculated the binid for a particle it also stores the index of that particle as an integer in the sort\_values array. 

		\subsection{Sorting Key/Value Pairs}
		The key/value pair sorting is done using the \gls{gls:thrust} library sort\_by\_key template function. This function is provided by NVIDIA with \gls{gls:CUDA}. The \gls{gls:thrust} sort is a radix sort that has been optimized for NVIDIA GPUs\cite{NVIDIACorporation2011a}. The snippet of the sort code used in SCEPTIC3DGPU is shown in figure \ref{fig:thrust_sort}.

\begin{figure}
\begin{lstlisting}[frame=single]
	// wrap raw device pointers with a device_ptr
	thrust::device_ptr<ushort> thrust_keys(binid);
	thrust::device_ptr<int> thrust_values(particle_id);

	// Sort the data
	thrust::sort_by_key(thrust_keys,thrust_keys+nptcls,thrust_values);
	cudaDeviceSynchronize();
\end{lstlisting}
\vspace{-0.4in}
\caption{Thrust Sort Setup and Call}
\label{fig:thrust_sort}
\end{figure}


		\subsection{Payload Move}
		The payload move is responsible for moving all of the particles from their old locations in memory to the new sorted locations. The idea is simple, each thread represents a slot on the sorted particle list. Threads read in an integer, the particleID, from the values array that was sorted using the binid's. This integer is the location of a given threads particle data in the unsorted list. Data at index particleID is read in, and stored in the new list at index threadID. While the idea is simple, this algorithm would require a completely separate copy of the particle list, a lot of wasted memory. However, since the particle list is set up as a structure of arrays, there is something that can be done to significantly reduce the memory requirements. The method, outlined in algorithm \ref{alg:payload_move} reorders only a single element of the particle list structure at a time. 

\begin{algorithm}
	\caption{GPU Payload Move}
	\label{alg:payload_move}
	\begin{algorithmic}
		\FORALL{ $member \in XPlist$}
		\STATE float* idata = member
		\STATE float* odata = XPlist.buffer
		\STATE \textbf{reorder\_data}(odata,idata,particleIDs)
		\STATE member = odata
		\STATE buffer = idata
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
  		
		Essentially the idea is that a great deal of memory can be saved by statically allocating a ``buffer'' array that is the same size as each of the data arrays. During the payload move each data array is sorted into the buffer array. Some pointer swapping is performed, the old buffer array becomes the new data array, and the old data array becomes the buffer for the next data array. For SCEPTIC3DGPU this implementation of the payload move only increases the particle list size by about 8.6\%.



	\section{Poisson Solve}
The field solve used in SCEPTIC3D involves solving the electrostatic Poisson equation:

\begin{equation}
\nabla^2\phi=\frac{n_e-n_i}{\Lambda_{De}^2}
\label{eqn:poisson_eqn1}
\end{equation}

Where $\phi$ is the electrostatic potential, $n_{e,i}$ the electron, ion charge densities normalized to the background charge density $N_\infty$, and $\Lambda_{De}$ is the unperturbed electron Debye length $\Lambda_{De} = \sqrt{\varepsilon_0T_e/N_\infty e^2}$. Using the SCEPTIC3D approximation that the electron density can be taken to be a Boltzmann distribution, and normalizing $\nabla$ to $1/R_p$ the probe radius, the Poisson equation can be rewritten as 

\begin{equation}
\nabla^2\phi=\frac{exp(\phi)-n}{\lambda_{De}^2}
\label{eqn:poisson_eqn2}
\end{equation}

We are changing $n_i$ to $n$ in order to avoid confusion with the use of $i$ as a spatial index. Assuming that the potential varies little from one time step to the next we can expand equation \ref{eqn:poisson_eqn2} about the known potential $\phi^*$ yielding
   
\begin{equation}
\nabla^2\phi_{i+1}=\frac{exp(\phi^*)[1+(\phi_{i+1}-\phi^*)]-n}{\lambda_{De}^2}
\label{eqn:poisson_eqn3}
\end{equation}

Equation \ref{eqn:poisson_eqn3} can be linearized into the form $A\phi+\omega = \sigma$ using finite volumes. Following the derivation for the linear operator $A$ from \cite{Patacchini2010} we get:


\begin{align}
(A\phi)_{i,j,k} &= a_i\phi_{i+1,j,k}
		  -b_i\phi_{i-1,j,k}
		  +c_{i,j}\phi{i,j+1,k}
		  -d_{i,j}\phi_{i,j-1,k} \nonumber \\
		  &\qquad{} +e_{i,j}(\phi_{i,j,k+1}-\phi_{i,j,k-1})
		  -[f_{i,j} + exp(\phi_{i,j,k}^*)]\phi_{i,j,k}
\label{eqn:psolve}
\end{align}

The full derivation of equation \ref{eqn:psolve} along with the definitions of the vectors $a$ through $f$ can be found in \cite{Haakonsen2011}.

Now that we have our linear operator defined, along with the vectors $\omega$ and $\sigma$ we can see that this is a simple sparse linear system. These system is fairly easy to solve using any sparse linear system solving scheme. One of the easier schemes to implement on the GPU is the preconditioned biconjugate gradient method \emph{PBCG}. This method is easy to parallelize due to the simplicity of the operations involved, namely \emph{PBCG} consists solely of sparse matrix-vector products, vector dot products, and multiplication of vectors by a scalar. All of these operations can be implemented efficiently on the GPU. 

We can make an additional optimization of the PBCG solver by recognizing that only one element of our linear operator $A$ changes every time step. This means that we can make use of some of the read-only memory spaces on the GPU, such as texture memory. Since texture memory is cached spatially in its own on-chip cache, we hope that we can significantly reduce the number of global memory transactions for all of the matrix multiplications. We ended up implementing two versions of the Poisson solve, one that stores both $A$ and $\phi^*$ as textures, and one that stores them in global memory. The version to be used is determined at compile time with the use of pre-processor macros. 
The only concern with the texture implementation would be the cost of calling cudaBindTextureToArray(). However, since only one diagonal of $A$ and and $\phi^*$ change over the course of the time step, the binding cost should not be an issue. Using textures for arrays in other parts of the code is also possible. Since we are using textures for the potential in the field solve it is very simple to also use a texture based potential for the particle advancing. The actual gain of using textures will be discussed in chapter \ref{ch:performance}.

We will not develop GPU implementations of the pre-processing steps required to prepare the ion density $n$ from the particle-density interpolation since those steps do not represent a significant computational cost. Additionally, the particle-density array already exists in host memory due to the fact that an \gls{ac:mpi} reduce must be performed to gather this array from multiple GPUs. Since this data is already in host memory, and the computational time is low, there is very little reason to rewrite these routines for the GPU.  


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Particle List Advance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
Moving the particles on the grid starts with determining the acceleration of the particle. This is calculated by interpolating the gradient of the potential, $\nabla\phi$, from the spherical mesh using the same methods as the cpu code. The new position of the particle is then calculated using the leap-frog method:

\begin{equation}
\begin{aligned}
\mathbf{v}' &= \mathbf{v}+\mathbf{a}\Delta t \\
\mathbf{x}' &= \mathbf{x}+\mathbf{v}'\Delta t
\end{aligned}
\end{equation}
 Additional details of the particle advance can be found in reference \cite{Patacchini2007} section 3.1.2.
		
While the implementation of the basic physics of the particle advance remains the same, there were several issues. Quickly determining whether a particle has crossed one of the domain boundaries, contributing to diagnostic outputs, and handling reinjections were the main issues. 

		\subsection{Checking Domain Boundaries}
In order to correctly contribute to the diagnostic outputs, the location where a particle left the domain must be known. This means that the process of checking whether or not a particle has left the grid must also calculate the position of the particle when it crossed the boundary. Since the boundaries are both spheres, we can apply a very common ray tracing technique used to calculate a line-sphere intersection. 

The equation for the path of a particle with initial position $\mathbf{p}_i$ and final position $\mathbf{p}_{i+1}$ is given by;

\begin{equation}
 \mathbf{l} = \frac{\mathbf{p}_{i+1} - \mathbf{p}_{i}}{\|\mathbf{p}_{i+1} - \mathbf{p}_{i}\|}
\mathbf{x} = d\mathbf{l}
\label{eqn:p_direction}
\end{equation}

Combining equation \ref{eqn:p_direction} to the equation of a sphere with radius $r$ centered at the origin, and expanding we get:

\begin{equation}
\begin{aligned}
\|d\mathbf{l} - (-\mathbf{p}_i)) \|^2  &= r^2 \\
d^2\mathbf{l}^2 + 2d(\mathbf{l}\cdot \mathbf{p}_i) + \mathbf{p}_i^2 &= r^2
		\end{aligned}
\end{equation}

We can then use the quadratic equation to solve for $d$. There are a few qualifiers, namely, $(\mathbf{l}\cdot \mathbf{p}_i)^2 - \mathbf{p}_i^2 + r^2 \geq 0$. The distance to the nearest intersection is simply the smallest positive solution of $d$. A particle leaves the domain if $d^2 \le \|\mathbf{p}_{i+1} - \mathbf{p}_{i}\|^2$. If this condition is true, then the particle's new position $p_{i+1}$ is changed such that $d^2 = \|\mathbf{p}_{i+1} - \mathbf{p}_{i}\|^2$. The particle's position is changed in order to accommodate some of the diagnostic outputs discussed in section \ref{sec:diagnostic_outputs}. 



		\subsection{Handling Reinjections}
Once it has been determined that particles have left the grid, new particles must be reinjected to replace them. In the serial version of the code this is handled by simply calling a reinjection subroutine that determines the new particle's position and velocity. Once the new position and velocity has been found, the particle is moved for the remainder of the time step, and replaced by a new particle if the reinjected particle leaves the domain.  

Performing reinjections in this manner on the GPU would introduce very large divergences in warp execution as well as very uncoalesced memory accesses. Eliminating the warp divergences would require that all of the threads in a warp be operating on reinjected particles. Reducing the uncoalesced memory accesses would require that all of the reinjected particles be adjacent in memory.  Since we already have an object with methods that can move a list of particles and handle reinjections, all we really need is some method by which we can efficiently and reversibly ``pop" a subset of the particles in the main list to a secondary list. From there we can perform the particle advance on the secondary list, and place the results back in the empty particle slots in the main list. The resulting advancing algorithm is as follows:

\begin{algorithm}
	\caption{Particle Advancing Algorithm}
	\label{alg:advancing}
	\begin{algorithmic}
		\STATE // Update The particle positions and check domain boundaries
		\STATE $\textbf{GPU\_Advance}(particles,mesh,Exit\_Flags)$
		\STATE
		\STATE \textbf{Prefix\_Scan}(Exit\_Flags)
		\STATE
		\STATE nptcls\_reinject = Exit\_Flags[nptcls-1]
		\STATE
		\IF{nptcls\_reinject $>$ 0}
			\STATE
			\STATE reinjected\_particles.allocate(nptcls\_reinject)
			\STATE
			\STATE \textbf{Stream\_Compact}(particles $\subset$ exited $\rightarrow$ reinjected\_particles)
			\STATE
			\STATE // Recursively call this algorithm on the reinjected particles
			\STATE reinjected\_particles.advance()
			\STATE
			\STATE \textbf{Stream\_Expand}(reinjected\_particles $\rightarrow$ exited $\supset$ particles)
		\ENDIF
		\STATE
		\STATE return
	\end{algorithmic}
\end{algorithm}

A graphical representation of algorithm \ref{alg:advancing} can be seen in figure \ref{fig:padvnc}, which also includes the handling of collisions. For the handling of both collisions and reinjections four separate steps are needed. Step (1) determines whether or not a particle will be colliding, when the collision happens, and flags that particle in the list. Step (2) advances the particle's positions and velocities, and flags all particles that leave the simulation domain. Particles undergoing collisions in step (2) are only advanced to the time of collision, and particles that leave the domain are advanced to the point where they exit the domain. Particles that are flagged for a collision, but leave the domain before the collision takes place do not undergo a collision, but are flagged for reinjection. At this point we have several groups of particles, two that require special operations and additional advancing, and a third that is completely finished. 
In step (3) we separate these groups of particles and into compacted collision and reinjection lists, and a sparse main list. The compacted lists are operated by the collision and reinjection operators respectively and then moved recursively. 
Once the recursive moves have been completed the particles in the two sub-lists are placed back in their original places in the main list by step (4). It should be noted that collisions have not yet been implemented in the GPU code, but the data handling infrastructure needed to support them as shown in figure \ref{fig:padvnc} has been implemented. 
Since the collision and reinjection operators are completely independent, we could make use of multiple \gls{gls:CUDA} ``streams" such that the two processes could be carried out asynchronously on the same GPU. 

\begin{figure}
\begin{center}
\input{implementation/reinject_collide.tex}
\end{center}
\caption[Illustration of GPU particle advancing algorithm]{Illustration of GPU particle advancing algorithm with handling of reinjections and collisions. All of the particles to be advanced begin in the same list. Prior to updating the particle's positions and velocities, step (1) determines which particles will undergo collisions, and when those collisions will occur. Between steps (1) and (2) the particle's positions and velocities are advanced. After the advance some particles will have left the domain, these particles are flagged in step (2), with exit flags overriding collision flags. After step (2) two stream compactions are performed in order to condense the re-injection and collision lists. In step (3) the collision and reinjection operators are applied to these two lists respectively, and the entire move method is called recursively on each of the lists. Once all recursive steps have finished the collision and re-injection lists are merged back into the main array in step (4). }
\label{fig:padvnc}
\end{figure}

Compacting some subset of a parent list is a fairly easy parallel operation called stream compaction. Stream compaction, shown in figure \ref{fig:stream_compact} is a process by which a random subset of a list can be quickly copied to a new list in parallel. It only works for some binary condition, such as an array of length nptcls, where each element is 1 for particles that have left the domain, and 0 for all others. For each `true' element taking the cumulative sum of all preceding elements yields a unique number that can be used as an index in a new array. 

\begin{figure}
\begin{center}
\includegraphics[width=4in]{implementation/stream_compact.jpg}
\end{center}
\caption[Stream Compaction]{Stream Compaction from GPU Gems 3 \cite{Harris2007}}
\label{fig:stream_compact}
\end{figure}



The new positions and velocities for reinjected particles are taken from a precalculate pool of new particles. This pool is approximately 1/10\textsuperscript{th} the size of the main particle list, and is repopulated prior to every advance step. Prior to the first advance step the entire pool is populated, but subsequent steps only refill slots that have been used in reinjections. This ``pool" method was chosen over implementing the reinjection calculations on the GPU for reasons of code maintenance. There are currently several different reinjection schemes that SCEPTIC3D can call. Maintaining versions of those routines in both fortran and \gls{gls:CUDA} would be more cumbersome than the small benefit that calculating them on the GPU would provide. 

		\subsection{Diagnostic Outputs}
		\label{sec:diagnostic_outputs}
	There are several outputs from the particle advancing routine that are desirable to keep in the GPU code. These outputs include;


\begin{itemize}
\singlespacing
\item Total momentum transferred to the probe.
\item Total momentum lost to particles leaving the system.
\item Total momentum added to the system from reinjections.
\item Total energy transferred to the probe. 
\item Total number of particles lost to the probe. 
\item Spatial distribution of energy transferred to the probe. 
\item Spatial distribution of momentum transferred to the probe. 
\item Spatial distribution of density transferred to the probe. 
\end{itemize}


	For the single value quantities, such as the quantities of total momentum, energy, and particle count are tallied using atomic operations in shared memory. Since the fraction of particles that leave the domain during a timestep is small, $\leq 7\%$, we can just use conditional statements within the primary move kernel to perform the tallies. While this is not strictly optimal, it does not have a very large impact on the overall performance of the move kernel. 






