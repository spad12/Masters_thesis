
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Over the past century humanity has become increasingly dependent on the 4th state of matter, plasma. Attaining a better understanding of plasma behaviour and interaction is critical to developing faster computer chips, creating new sources of energy, and expanding humanities influence among the stars. One important subset of plasma behaviour is how plasmas interact with solid objects such as dust particles, probes, and bodies traveling through space. These interactions can be very difficult to explore experimentally, and therefore must be modeled. 
	
A plasma's behaviour is heavily influenced by the collective electric and magnetic fields generated by the individual particles that comprise the plasma. This means that plasma behaviour is essentially a very large n-body problem, where for moderately dense plasmas n can be on the order of $10^{20}$ per cubic meter. No computer in existence can store the information for $10^{20}$ particles per cubic meter for large geometries, and calculating the interactions of every particle in the set with every other particle would be prohibitively long. The solution to this problem is to model only a subset of the particles. The modeled behaviour of these particles and their contributions to magnetic and electric fields can be used to statistically infer the behaviour of the rest of the plasma, essentially from first principles. This method is called particle-in-cell (PIC), and operates by moving particles on a potential grid and updating that potential with the new particle density at every timestep. 

The PIC method begins with an initial particle distribution, $\mathcal{P}(\mathbf{x}_i,\mathbf{v}_i)$. which is interpolated to the simulation mesh vertices in order to generate the charge and current density, $\mathcal{V}(\rho_j,\mathbf{J}_j)$. Integrating the charge and current densities defines the electric and magnetic fields, $\mathcal{V}(\mathbf{E}_j,\mathbf{B}_j)$ at the mesh vertices. From here the force, $\mathbf{F}_i$ on each particle is interpolated from $\mathbf{E}_j$ and $\mathbf{B}_j$. The force is then used to advance the positions and velocities of every particle. Following the advancing step, new particles must be reinjected to replace those that left the simulation domain. Collisions can be included after the reinjection step, although not all codes choose to include them. Finally new particle distribution function is interpolated to the mesh and the entire process starts again. Figure \ref{fig:pic_flowchart} shows the typical flow for this kind of electromagnetic PIC code.\cite{Verboncoeur2005}
\begin{figure}
\begin{center}
\input{introduction/pic_flowchart.tex}
\end{center}
\caption[Flow schematic for the PIC method.]{Flow schematic for the electromagnetic PIC method. Monte Carlo collisions are optional and not included in all codes.}
\label{fig:pic_flowchart}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
	\section{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	The PIC method is very good at modeling complicated plasma behaviour, however this method relies on tracking very large numbers of particles for good statistics. In order to achieve ``good'' statistics PIC codes employ millions to billions of particles, which means that these codes require a very large amount of computation time for each timestep. Running billions of particles on a single core for thousands of timesteps is not really feasible, it simply takes too long to compute a solution. 
	
	One way to reduce the total run time of PIC codes is to parallelize them. Since PIC codes operate on the fact that the potential changes little over the course of a single timestep, each particle can be assumed to be independent of its neighbors. This leads to a situation that is trivially parallel. In theory a machine with a million processors could run every particle on a separate processor. This of course assumes that the majority of the computational complexity lies in moving the particles and that communication between processors is very fast.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{GPUs vs CPUs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	The ideal computing system for a particle in cell code should have a large number of relatively simple processors with very low communication overhead. Traditional CPUs are just the opposite of this. CPUs tend to have 4-8 complicated processors that are very good at performing large operations on small sets of data, but very slow at communicating between multiple processors. CPUs are designed to be able to actively switch tasks on the fly, which makes them very good at simultaneously running web-browser, decoding a video, and playing a video game. However, this flexibility requires a large number of cycles to switch between tasks, and a large amount of cache to store partially completed tasks.

Graphical processing units, or GPUs, forgo the flexibility of CPUs in favor of raw processing capability. Reducing the size of the cache and employing single instruction multiple data (SIMD) parallelism allows GPU manufacturers to combine hundreds of processors on a single chip. In order to supply enough data to keep hundreds of processors busy, GPUs also have a very large data channel between processors and DRAM. All of these features are chosen to create a math processor that excels at operations where each processor operates on data that is invisible to the other processors. These features give GPUs a significant raw floating point performance advantage over CPUs, as shown in figure \ref{fig:gpu_vs_cpu}. 


\begin{figure}
\begin{center}
\includegraphics[width=5in]{introduction/gpu_vs_cpu.png}
\end{center}
\caption[Performance comparison of GPUs vs CPUs.]{Performance comparison of GPUs vs CPUs. GPU performance is continuing to increase at a very rapid pace.\cite{NVIDIACorporation2011}}
\label{fig:gpu_vs_cpu}
\end{figure}

The hardware in GPUs is tailored to excel at performing tasks such as ray-tracing, which is very similar to particle moving. Therefore it is by no means unreasonable to conclude that GPUs can be very good PIC code processors. The advantages that GPUs have over CPUs for scientific computing include:

\begin{itemize}
	\item Higher performance per unit cost.
	\item Higher performance per watt.
	\item Easier to upgrade.
	\item GPUs are still improving with Moore's law.
\end{itemize}

All of their advantages are observed when comparing the CPU and GPU versions of the same PIC code. While these advantages are very promising there are also several disadvantages to GPU computing:

\begin{itemize}
	\item Increased code complexity.
	\item Smaller memory space.
	\item Smaller cache.
	\item Slow communication between CPU and GPU.
	\item Algorithms dependent on hardware configuration.
\end{itemize}

The key to developing efficient PIC algorithms that utilize GPUs lies in balancing the work between the two architectures. Some operations will be easier to implement on the CPU and be just as fast as the GPU, while others will be significantly faster on the GPU. Partitioning the code between different architectures outlines a very important aspect of parallel computing; multiple levels of parallelism.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Multiple Levels of Parallelism}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Currently most parallelization is done by dividing a task between a number of threads on different CPUs, and using an communication and synchronization API such as \gls{ac:mpi} to allow those threads to communicate. This network of threads has a master node, usually node 0, which orchestrates the communication between other nodes. This is analogous to how a single CPU-GPU system operates. The CPU is the ``Master'' and serves as a communication hub for groups of execution threads on the GPU called thread blocks. Each thread block is itself a cluster of threads that can communicate through a memory space aptly named ``shared memory''. When all of these systems are used together the resulting architecture has multiple levels of parallelism. The levels parallelism of a multi-GPU architecture are shown in figure \ref{fig:multiparallel}.

\begin{figure}
\begin{center}
\includegraphics[width=5in]{introduction/multi_parallel.png}
\end{center}
\caption[Multiple levels of parallelism.]{Multiple levels of parallelism. (1) Cluster of systems communicating through a LAN. (2) Multiple GPUs per system communicating through system DRAM. (3) Multiple streaming multiprocessors per GPU execute thread-blocks and communicate through GPU global memory. (4) Multiple \gls{gls:CUDA} cores per multiprocessor execute thread-warps and communicate through on chip shared memory. }
\label{fig:multiparallel}
\end{figure}

The point is that multiple domain decompositions must be performed in order to fully utilize the capabilities of this system. The coarse decomposition is very similar to that used for \gls{ac:mpi} systems, but the fine decomposition can be very different due to the significantly higher memory bandwidth and smaller cache of GPUs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Parallelization Opportunities in PIC Codes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\begin{center}
\input{introduction/parallel_pic_flowchart.tex}
\end{center}
\caption[Parallization opportunities in the PIC method]{Flow schematic for the electrostatic PIC method with parallelizable steps highlighted. (1) and (2) are possible paths that the code can take depending on whether or not collisions are used. (A) Particle steps are embarassingly parallel. (B) Particle to Mesh interpolation steps are difficult to parallelize. (C) Field solving step can make use of parallelized linear algebra libraries.}
\label{fig:pic_flowchart_parallel}
\end{figure}

Most of the steps in the PIC method can be parallelized, although some steps are more difficult than others. Figure \ref{fig:pic_flowchart_parallel} highlights the steps of the PIC method that are parallelizeable, and groups them according to how they can be parallelized. The particle steps, group (A) in the figure, are probably the easiest steps to implement in parallel. In theory every particle could be advanced by a separate processor or thread. In the case of GPUs it is entirely feasible to assign a single thread to every particle. There are several issues that arise when implementing steps that contain a large amount of execution divergence on the GPU. 
Execution divergence occurs whenever the computation path for two particles requires different instructions, such as if one particle must evaluate the contents of an \emph{if} statement, while another does not.
The particle loss / re-injection step, and the collision step are two such subroutines in which the execution paths of particles that are being lost, re-injected, or colliding will diverge greatly from the majority of the particles. In these cases it is best to operate on subsets of the particle list in which all particles of the subset follow the same execution path. There are ways in which subsets of the particle list can be operated on efficiently without large amounts of execution divergence. 

Parallelizing the particle to mesh step $((\mathbf{x}_i,\mathbf{v}_i)\rightarrow \rho_j)$, (B) in figure \ref{fig:pic_flowchart_parallel}, is trickier, depending on the amount of memory available per thread. In the case of multi-CPU implementations, each thread has enough memory to store a copy of the entire grid. This thread can process a particle that exists anywhere on the grid, and map its contribution to the grid without memory conflicts. The case is very different for GPUs, which have insufficient memory to support replicating the entire grid for every thread. Therefore the key to parallelizing the particle-to-mesh interpolation step on the GPU is determining way to efficiently avoid memory conflicts, using some kind of \gls{gls:atomicmemoryoperation} or domain decomposition. 

The last step (C), the field solve $(\rho_j \rightarrow \mathbf{E}_j)$, can be fairly straightforward to parallelize. The field solve involves integrating Maxwell's equations over the entire grid. In the case of electrostatic codes this is simply solving Poisson's equation. Poisson's equation is a straightforward linear equation that can be solved using a number of parallel $Ax=b$ solvers, popular ones being preconditioned bi-conjugate gradient solvers and spectral solvers.

The vast majority of the PIC algorithm is well suited for implementation on the GPU. There are some challenges to overcome, such as how to perform the charge assign, and how to efficiently handle re-injections and collisions. Currently GPU computing is in its infancy, and the solutions to these challenges are evolving rapidly. In the following section we will look at some of the solutions that others have developed to overcome these issues. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\section{GPU PIC Code Development}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Before we dive into past research into implementing PIC on GPUs it would be useful to know a little more about general purpose graphical processor (GPGPU) computing. Currently there are two frameworks for writing GPGPU programs, the Open Computing Language (OpenCL) initially developed by Apple Inc., and the NVIDIA Corporation's Compute Unified Architecture \gls{gls:CUDA}.\cite{NVIDIACorporation2012d,NVIDIACorporation2011} Each framework has its own strengths and weaknesses. \gls{gls:CUDA} is a software environment that allows developers to use C as a high-level programming language. Developing in \gls{gls:CUDA} C is straightforward and is very well documented and supported by NVIDIA. The \gls{gls:CUDA} toolkit and SDK provided by NVIDIA include decent debugging and profiling tools as well as a large number of fully GPU accelerated libraries. These libraries include:

\begin{itemize}
\item \gls{gls:thrust} (template and algorithm library)\cite{NVIDIACorporation2011a}
\item Cula (linear algebra library)
\item CUBLAS (Basic Linear Algebra Subprograms)\cite{NVIDIACorporation2012}
\item CUFFT (Fast Fourier Transform (FFT) Library)\cite{NVIDIACorporation2012a}
\item CUSPARSE (Sparse linear algebra routines)\cite{NVIDIACorporation2012c}
\item CURAND (high-quality pseudo-random and quasi-random numbers)\cite{NVIDIACorporation2012b}
\end{itemize}

The primary downside to \gls{gls:CUDA} is that it is restricted to NVIDIA GPUs, and cannot be used for other hardware platforms. OpenCL on the other hand supports multiple computing platforms and has been adopted by Intel, Advanced Micro Devices, Nvidia, and ARM Holdings. However, developing in OpenCL can be somewhat cumbersome compared to \gls{gls:CUDA}, especially when it comes to debugging and profiling. OpenCL also lacks mature libraries similar to those provided with \gls{gls:CUDA}. For these reasons, and the fact that the primary compute-specific cards are the NVIDA Tesla cards, most GPU PIC code development has been done using \gls{gls:CUDA}.\cite{NVIDIACorporation2012d,NVIDIACorporation2011}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\section{Current Status of GPU PIC codes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

						Some work on efficient GPU based PIC codes has already been done. This past work will be briefly introduced here and discussed in depth in chapter \ref{ch:design}. One of the earliest efforts in developing plasma PIC simulations on the GPU is that of George Stantchev et al. In 2008 Stantchev et al published a paper in the \emph{Journal of Parallel Distributed Computing}\cite{Stantchev2008} outlining the development of a fast density accumulation method for PIC implementations on the GPU. Stantchev's paper was one of the first to identify the issue of parallelizing the density update and develop a very efficient solution. The solution involves defining clusters of grid cells and sorting the particle list according to which cluster they belong to. The cell clusters are small enough to fit into \emph{\_\_shared\_\_} memory and updated pseudo atomically using a now obsolete thread tagging technique. The simplicity and efficiency of this method has led to its use in many other GPU PIC implementations. Stantchev also identified the issue of the particle sort step required by their density accumulation method. Developing an optimized sort has since become one of the biggest challenges for GPU PIC implementations. 

Also in 2008, Dominique Aubert et al published their development of a GPGPU galactic dynamics PIC code.\cite{Aubert2009}. Aubert et al adapted their entire code for GPUs and tested several field-solving and density-accumulation techniques. The first density accumulation technique relied on the radix sort provided by the \gls{gls:CUDA} Data Parallel Primitives (CUDPP) library to order the particle data and construct a histogram. The second technique used the GPU to find the nearest cells for each particle and then construct the density histogram on the CPU. The details concerning how these histograms were constructed are not disclosed in the paper. As for field solving techniques, GPU versions of both FFT and multi-grid Poisson solvers were used. The FFT solver made use of the CUFFT API provided by the \gls{gls:CUDA} Toolkit. The multi-grid (MG) solver was a from scratch GPU implementation of the MG solver outlined in \cite{NumericalRecipes}. Aubert et al achieved speed ups of 40x for the FFT and 60x for MG, and approximately 20x for the velocity and position updates, but the histogramming was on the order of 5-10x \emph{slower} on the GPU. They acknowledge that their histogram method is too slow, and propose using an improved technique following that developed by Stantchev et al.

A further step in GPU PIC codes was taken in 2010 by the simulation code PIConGPU\cite{Burau2010}, developed by Heiko Burau et al. PIConGPU was one of the first multi-GPU PIC implementations involving multiple nodes. PIConGPU is a fully relativistic, 2D electromagnetic code used to simulate the acceleration of electrons in an under-dense plasma by a laser-driven wakefield. Burau et al also identified the issue of parallelizing charge and current density accumulation on SIMD architectures and came up with a unique solution, a linked particle list. This linked particle list is one of the core features of PIConGPU and is geared towards maintaining localized data access for each kernel. In order to facilitate scalability over multiple nodes the simulation volume is domain decomposed and distributed across multiple GPUs. Each sub-domain is bordered by a region of guard cells to facilitate particle transfers between sub-domains. These guard cells overlap with adjacent domains and serve as a buffer for particles moving from one sub-domain to the next. The results of this research indicated that single GPU PIC algorithms can be scaled to multi-node clusters despite high host-device communication latency.

In March, 2010 NVIDIA released the first GPUs utilizing the Fermi architecture. Fermi boasted several key improvements for scientific computing over the G80 and GT200 architectures. These improvements included \cite{NVIDIACorporation2009}:

\begin{itemize}
\singlespacing
\item Configurable L1 and unified L2 caches.
\item 8x the double precision performance of the GT200 architecture. 
\item Error-Correction Code (ECC) support. 
\item Improved \gls{gls:atomicmemoryoperation} performance.
\item Full IEEE 754-2008 32-bit and 64-bit arithmetic floating point precision compliance. 
\end{itemize}

The significant increase in double precision performance and addition of ECC support are desirable for scientific computing on GPUs. With the launch of Fermi, work on GPU based PIC codes increased significantly. Another 2D relativistic GPU PIC code was developed by Paulo Abreu et al in late 2010.\cite{Abreu2011} Abreu et al chose an approach to the charge and current accumulation which was in some regards opposite of that employed by Stantchev et al. This approach was based on using pseudo \glspl{gls:atomicmemoryoperation} that actually consisted of two \textbf{atomicExchange}$()$ operations. In order to minimize atomic collisions the particle list is ordered such that the probability that two threads within a \gls{gls:warp} were operating on particles in the same cell is low. The initial particle distribution is prepared in way to minimize this probability, but this distribution can be upset throughout the simulation. If the distribution degradation reaches a certain threshold then a redistribution of the particle list takes place. This redistribution was handled by a sorting operation, using the CUDPP radix sort, and a \emph{stride} distance, or the distance in particle index between particles handled by one thread and the next. The resulting code had a relatively fast particle advance but the current deposition was rather slow. The current deposition step constituted approximately 67\% of the code runtime for 1.2 million particles on a $128^2$ grid.

One of the more unique GPU PIC implementations was developed by Rejith Joseph et al and published at the 2011 \emph{IEEE International Parallel \& Distributed Processing Symposium}.\cite{Joseph2011} This code, a GPU implementation of the XGC1 code differed from previous codes in two ways. First, XGC1 uses an asymmetric triangular mesh, unlike most GPU PIC codes which use symmetric rectangular meshes. Second, there are no limits imposed on particle movement, which makes the lessons taught by the code more applicable in general.

The first point, the use of a triangular mesh is interesting because most of the time a symmetric rectangular mesh is preferred on the GPU. Triangular meshes can be more efficient at simulating irregular domains by using finer spatial discretization for important regions.  As in other PIC codes, in order to facilitate a fast particle to grid interpolation it was important that all of the triangles that a thread-block might access fit into the limited \emph{\_\_shared\_\_} memory. This limit, imposed by the size of \emph{\_\_shared\_\_} memory, necessitated that the primary mesh be partitioned into smaller regions. Joseph et al tried three different partitioning strategies, density based, uniform, and non-uniform. The density based strategy partitioned the mesh such that each region had approximately the same triangle density. The uniform approach divided the mesh into large uniform rectangular regions. Lastly the non-uniform partitioning was a sort of hybrid of the uniform and density approaches, using rectangular regions that varied in size depending on the granularity of the triangle density.\cite{Joseph2011}

The goal of these partitioning schemes was to take care of load balance on the GPU. As it turns out, the automatic load balancing provided by the GPU hardware is very good. When Joseph et al compared triangle search times between the different partitioning schemes the uniform scheme was the fastest for all numbers of thread blocks, but for very large numbers of blocks the Uniform and the Non-Uniform schemes converged to roughly the same execution time. This brings up a very interesting point that can teach us a valuable lesson; namely, the GPU tends to like very simple problems and often times increasing the complexity results in reduced performance. While complicated performance enhancing techniques work well on serial architectures, often they will only hurt you when implemented on the GPU. 

The final code that we will look at was published in the \emph{Journal of Computational Physics} by Xianglong Kong et al in 2011.\cite{Kong2011} This code was a 2D3V fully relativistic electromagnetic code that achieved speed-up factors of 81x and 27x over the CPU runtime for cold plasma runs and extremely relativistic plasma runs respectively. This code approached the issue of maintaining an organized particle list very differently than previous codes. The approach used in this code is more akin to message passing than sorting, and will be explained in more detail in chapter \ref{ch:design}. The performance of this code greatly depended on the fraction of particles that crossed from one sub-region into the next, or the crossing fraction $\eta$. The achieved run-time per particle-step was 4.6 ns for $\eta=0.5\%$, and up to 9.15 ns for $\eta=7.5\%$. 

		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{SCEPTIC3D}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now that previous works in the realm of GPU PIC codes have been discussed, it is time to introduce SCEPTIC3D, the code that we chose to implement a GPU version of. SCEPTIC3D is three dimensional hybrid PIC code specifically designed to solve the problem of ion flow past a negatively biased sphere in a uniform magnetic field. The current version of the code was derived from the 2D/3v code SCEPTIC which was originally written by Ian Hutchinson \cite{Hutchinson2002,Hutchinson2003,Hutchinson2005,Hutchinson2006}. The code is written in FORTRAN, and has been previously parallelized using standard \gls{ac:mpi} communication interface. Uses of SCEPTIC3D include exploring probe-plasma interactions, modeling the behaviour of particle grains in dusty plasmas, and determining the interactions between objects in space and their environment. As previously mentioned, the PIC method requires a large number of particles in order to reduce statistical noise. In the case of SCEPTIC3D, runs typically consist of 50 million particles.
Moving 50 million particles on a $64^3$ grid takes roughly 11 seconds per step using 4 cores of an Intel i7 930 processor. The fact that SCEPTIC3D has already been parallelized using \gls{ac:mpi} makes it far easier to develop a multi-GPU implementation. 

	\subsection{CPU Code Profiling}


\begin{figure}
\begin{center}
\input{introduction/sceptic3D_profile.tex}
\end{center}
\caption[Breakdown of SCEPTIC3D runtime costs]{Breakdown of SCEPTIC3D runtime costs by subroutine. This is for 12.5 million particles on a $64^3$ grid. Times are in ns per particle step.}
\label{fig:sceptic3D_profile}
\end{figure}

In figure \ref{fig:sceptic3D_profile} we break down the runtime of SCEPTIC3D into the main subroutines. This figure indicates that porting the charge assign and particle advancing steps should be the main priority. Of course, if we hope to get a performance boost of more than 3x for both the particle advancing and charge assign steps, then we will have to worry about the field solve as well. However, it should be noted that the balance between the particle and field costs depends on the number of particles per cell. In the case of figure \ref{fig:sceptic3D_profile} we used roughly 48 particles per cell, generally a larger fraction is preferable in order to reduce particle noise. 

%\begin{figure}
%\begin{center}
%\includegraphics[width=4in]{introduction/not_finished.pdf}
%\end{center}
%\caption{Flow schematic for the PIC method with sceptic subroutine names Need to make figure}
%\label{fig:pic_flowchart_sceptic}
%\end{figure}


The GPU implementation of SCEPTIC3D is very different from any other GPU PIC implementation. While we will focus a great deal on performance, we will also place a great deal of emphasis on implementation difficulty and applicability to future PIC implementations on the GPU. We will determine how critical the performance of the field solver is when the run time of other steps is significantly reduced. Lastly, we will look at how primarily graphics related GPU features can be utilized by PIC codes. 













