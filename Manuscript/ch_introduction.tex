
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Over the past century humanity has become increasingly dependent on the 4th state of matter, plasma. Attaining a better understanding of plasma behaviour and interaction is critical to developing faster computer chips, creating new sources of energy, and expanding humanities influence amoung the stars. One important subset of plasma behaviour is how plasmas interact with solid objects such as dust particles, probes, and bodies traveling through space. These interactions can be very difficult to explore experimentally, and therefore must be modelled. 
	
A plasma's behaviour is heavily influenced by the collective electric and magnetic fields generated by the individual particles that comprise the plasma. This means that plasma behaviour is essentially a very large n-body problem, where for moderately dense plasmas n can be on the order of $10^{20}$. No computer currently in existence can store the information for $10^{20}$ particles, and calculating the interaction of every particle in the set with every other particle would be prohibitvely long. The solution to this problem is to model only a subset of the true number of particles. The modeled behaviour of these particles and their contributions to magnetic and electric fields can be used to statistically infer the behaviour of the rest of the plasma, essentially from first princeiples. This method is called particle-in-cell (PIC), and operates by moving particles on a potential grid and updating that potential with the new particle density at every timestep. The flow of a general PIC code is shown in figure \ref{fig:pic_flowchart}. The PIC method is a very robust and straightforward scheme for modeling plasma behaviour, and is used extensively to model plasmas in complicated systems.

\begin{figure}
\begin{center}
\input{introduction/pic_flowchart.tex}
\end{center}
\caption{Flow schematic for the PIC method.}
\label{fig:pic_flowchart}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
	\section{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	The PIC method is very good at modeling complicated plasma behaviour, however this method still relies on tracking a very large number of particles for good statistics. In order to achieve ``good'' statistics PIC codes employ millions to billions of particles, which means that these codes can require a very large amount of computation time for each timestep. Running millions of particles on a single processor for hundreds of timesteps is not really feasible, it simply takes too long to compute a solution. 
	
	One way to reduce the total run time of PIC codes is to parallelize them. Since PIC codes operate on the fact that the potential changes little over the course of a single timestep, each particle can be assumed to be independent of its neighbors. This leads to a situation that is trivially parallel. In theory a machine with a million processors could run every particle on a seperate processor. This is of course assuming that the majority of the computational complexity lies in moving the particles and that comunication between processors is very fast.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{GPUs vs CPUs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	The ideal computing system for a particle in cell code should have a large number of relatively simple processors with very low communication costs. Traditional CPUs are just the opposite of this. CPUs tend to have 4-8 complicated processors that are very good at performing large operations on small sets of data, but very slow when it comes to communicating between multiple processors. CPUs are designed to be able to actively switch tasks on the fly. This makes them very good at simultaneously running web-browser, decoding a video, and playing a video game. However, this flexibility requires a large number of cycles to switch between tasks, and a large amount of cache to store partially completed tasks.

Graphical processing units, or GPUs, forgo the flexibility of CPUs in favor of more raw processing capability. Reducing the size of the cache and employing single instruction multiple data (SIMD) parallelism allows GPU manufactures to combine hundreds of processors on a single chip. In order to supply enough data to keep hundreds of processors GPUs also have a very large data channel between the processors and DRAM. All of these features are chosen to create a math processor that excels at tasks where each processor operates on data that is invisible to the other processors. These features give GPUs a significant raw floating point performance advantage over CPUs as seen in figure \ref{fig:gpu_vs_cpu}. 


\begin{figure}
\begin{center}
\includegraphics[width=4in]{introduction/gpu_vs_cpu.png}
\end{center}
\caption{Performance comparison of GPUs vs CPUs.}
\label{fig:gpu_vs_cpu}
\end{figure}

The hardware in GPUs is tailored to excel at performing tasks such as ray-tracing, which is very similar to particle moving. Therefore it is by no means unreasonable to conclude that GPUs can be very good PIC code processors. The advantages that GPUs have over CPUs for scientific computing include:

\begin{itemize}
	\item Higher performance per cost.
	\item Higher performance per watt.
	\item Easier to upgrade.
	\item GPUs still improving with Moore's law.
\end{itemize}

All of which are observed when comparing the CPU and GPU versions of the same PIC code. While these advantages are very promising there are also several disadvantages to GPU computing:

\begin{itemize}
	\item Increased code complexity.
	\item Smaller memory space.
	\item Smaller cache.
	\item Slow communication between CPU and GPU.
	\item Most developed GPU language is an extension of C.
	\item Algorithms can be very dependent on hardware configuration.
\end{itemize}

The key to developing efficient PIC algorithms that utilize GPUs lies in balancing the work between the two architectures. Some operations will be easier to implement on the CPU and be just as fast as the GPU while others will be significantly faster on the GPU. Partitioning the code between the different architectures begins to outline a very important aspect of parallel computing, multiple levels of parallelism.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Multiple Levels of Parallelism}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Currently most parallelization is done by dividing up a task between a bunch of threads on different CPUs, and using an interface such as MPI to allow those threads to communicate. This network of threads has a master node, usually node 0, which orchestrates the communication between the other nodes. This is analogous to how a single CPU-GPU system operates. The CPU is the ``Master'' and serves as a communication hub for groups of execution threads on the GPU called thread blocks. Each thread block is itself a cluster of threads that can communicate through a memory space aptly named ``shared memory''.

\begin{figure}
\begin{center}
\includegraphics[width=5in]{introduction/multi_parallel.png}
\end{center}
\caption{Multiple levels of parallelism. (1) Cluster of systems communicating through a LAN. (2) Multiple GPUs per system communicating through system DRAM. (3) Multiple streaming multiprocessors per GPU execute thread-blocks and communicate through GPU global memory. (4) Multiple cuda cores per multiprocessor execute thread-warps and communicate through on chip shared memory. }
\label{fig:multiparallel}
\end{figure}

The point hear is that multiple domain decompositions must be performed in order to fully utilize the capabilities of this system. The coarse decomposition is very similar to that used for MPI systems, but the fine decomposition can be very different due to the significantly higher memory bandwidth and smaller cache of GPUs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Parallelization Opportunities in PIC Codes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\begin{center}
\input{introduction/parallel_pic_flowchart.tex}
\end{center}
\caption{Flow schematic for the PIC method with parallelizable steps highlighted. Need to make figure}
\label{fig:pic_flowchart_parallel}
\end{figure}

Most of the steps in the PIC method can be parallelized. The particle advancing step is the most obvious step, but there are also ways to parallelize the charge assign and field solve. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Current Status of GPU PIC codes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

						Some work on efficient GPU based PIC codes has already been done. This past work will be briefly introduced here and discused in depth in chapter \ref{ch:design}. One of the earliest efforts in developing plasma PIC simulations on the GPU is that of George Stantchev et al. In 2008 Stantchev's group published a paper in the \emph{Journal of Parallel Distributed Computing} outlining the development of a fast density accumulation method for PIC implementations on the GPU. Stantchev's group was one of the first to identify the issue of parallelizing the density update and develop a very efficient solution. The solution involves defining clusters of grid cells and sorting the particle list according to which particle cluster they belong to. The cell clusters are small enough to fit into shared memory and updated pseudo atomically using a now obsolete thread tagging technique. The simplicity and efficiency of this method has led to its use in many other GPU PIC implementations. Stantchev's group also identified the issue of the particle sort step required by their density accumulation method. Developing an optimized sort has since become one of the biggest challenges for GPU PIC implementations. \cite{Stantchev2008}

Also in 2008, Dominique Aubert et al published their development of a GPGPU galactic dynamics PIC code. Aubert's group adapted their entire code for GPUs and tested several field solving and density accumulation techniques. The first density accumulation techniques used relied on using the radix sort provided by the CUDPP library to order the particle data and perform a histogram. The second technique used the GPU to find the nearest cells for each particle and then perform the histogram on the CPU. As for field solving techniques, GPU versions of both FFT and multi-grid Poisson solvers were used. The FFT solver made use of the CUFFT API provided by the CUDA Toolkit. The multi-grid solver was a from scratch GPU implementation of the MG solver outlined in \cite{NumericalRecipes}. Aubert et al achieved speed ups of 40x for the FFT and 60 for MG, approximately 20x for the velocity and position updates, but the histogramming was on the order of 5-10 times \emph{slower} on the GPU. They acknowledge that their histogramming method is too slow, and propose using an improved technique following that developed by Stantchev et al.\cite{Aubert2009}.

A large leap in GPU PIC codes was taken in 2010 by the simulation code PIConGPU, developed by Heiko Burau et al. PIConGPU was one of the first scalable GPU PIC implementations involving multiple nodes. PIConGPU is a fully relativistic, 2D electromagnetic code used to simulate the acceleration of electrons in an underdense plasma by a laser-driven wakefield. Burau et al also identified the issue of parallelizing charge and current density accumulation on SIMD architectures and came up with a unique solution, a linked particle list. This linked particle list is one of the core features of PIConGPU and is geared towards maintaining localized data access for each kernel. In order to facilitate scalability over multiple nodes the simulation volume is domain decomposed and distributed across multiple GPUs. Each subdomain is bordered by a region of guard cells to facilitate particle transfers between subdomains. The results of this research indicated that single GPU PIC algorithms can be scaled to multi-node clusters despite high host-device communication latency.\cite{Burau2010}

In March, 2010 NVIDIA released the first GPUs utilizing the Fermi architecture. Fermi boasted several key improvements for scientific computing over the G80 and GT200 architectures. These improvements included \cite{NVIDIACorporation2009}:

\begin{itemize}
\singlespacing
\item Configurable L1 and unified L2 caches.
\item 8x the double precision performance of the GT200 architecture. 
\item ECC support. 
\item Improved atomic memory operation performance.
\item Full IEEE 754-2008 32-bit and 64-bit precision. 
\end{itemize}

The significant increase in double precision performance and addition of ECC support are huge wins for scientific computing on GPUs. With the launch of Fermi, work on GPU based PIC codes increased significantly. Another 2D relativistic GPU PIC code was developed by Paulo Abreu et al in late 2010. Abreu et al chose an approach to the charge and current accumulation was in some regards opposite of that employed by Stantchev et al. This approach was based on using pseudo atomic operations that actually consisted of two \textbf{atomicExchange}$()$ operations. In order to minimize atomic collisions the particle list was ordered such that the probability that two threads within a warp were operating on particles in the same cell is low. The initial particle distribution is prepared in way to minimize this probability, but this distribution can be upset throughout the simulation. If the distribution degradation reaches a certain threshold then a redistribution of the particle list takes place. This redistribution was handled by a sorting operation, using the CUDPP radix sort, and a \emph{stride} distance. The resulting code had a relatively fast particle advance but the current deposition was rather slow. The current deposition step constituted approximately 67\% of the code runtime for 1.2 million particles on a $128^2$ grid.\cite{Abreu2011}

One of the more unique GPU PIC implementations was developed by Rejith Joseph et al and published at the 2011 \emph{IEEE International Parallel \& Distributed Processing Symposium}. 
This code, a GPU implementation of the XGC1 code differed from previous codes in two ways. 
First, XGC1 uses an asymmetric triangular mesh, unlike most GPU PIC codes which use symmetric rectangular meshes. Second, there are no limits imposed on particle movement. 

The first point, the use of a triangular mesh is interesting because most of the time a symmetric rectangular mesh is preferred on the GPU. As in other PIC codes, in order to facilitate a fast particle to grid interpolation it was important that all of the triangles that a thread-block might access fit into the limited shared memory. This limit imposed by the size of shared memory necessitated that the primary mesh be partitioned into smaller regions. Joseph et al tried three different partitioning strategies, density based, uniform, and non-uniform. The density based strategy partitioned the mesh such that each region had approximately the same triangle density. The uniform approach divided the mesh into large uniform rectangular regions. Lastly the non-uniform partitioning was a sort of hybrid of the uniform and density approaches, using rectangular regions that varied in size depending on the granularity of the triangle density. 

The goal of these partitioning schemes was to take care of load balance on the GPU.





 Several relativistic PIC codes have been developed for the GPU. The first of these, developed by Burau et al is a 2D electromagnetic code called PIConGPU. This was the first 


Burau et al developed a fully relativistic PIC code for a gpu cluster. 
		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Sceptic3D}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now that Sceptic3D is three dimensional hybrid PIC code specifically designed to solve the problem of ion flow past a negativley biased sphere in a uniform magnetic field. The current version of the code was derivied from the 2D/3v code SCEPTIC which was originally written by Hutchinson \cite{Hutchinson2002,Hutchinson2003,Hutchinson2005,Hutchinson2006}.

\begin{figure}
\begin{center}
\includegraphics[width=4in]{introduction/not_finished.pdf}
\end{center}
\caption{Flow schematic for the PIC method with sceptic subroutine names Need to make figure}
\label{fig:pic_flowchart_sceptic}
\end{figure}

	\subsection{CPU Code Profiling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\section{Basic Code Structure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

		
		\subsection{Charge Assign Details}
		
		\subsection{Poisson Solve Details}
		
		\subsection{Particle Advancing Details}
		

		

	\section{Overview of sceptic3Dgpu Goals}

		\subsection{Main Routines}

		\subsection{Supporting Routines}

		\subsection{Challenges to overcome}















